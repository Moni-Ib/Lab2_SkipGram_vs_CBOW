{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d7d6a8",
   "metadata": {},
   "source": [
    "___\n",
    "# <font color= #d4b1e6> **Laboratorio 2: Skip-gram vs CBOW – Word Embeddings from Scratch** </font>\n",
    "- <Strong> Nombre de los integrantes: </Strong>  <font color=\"blue\">`Sarah Lucía Beltrán, Priscila Cervantes Ramírez, Mónica Ibarra Herrera` </font>\n",
    "- <Strong> Materia: </Strong>  <font color=\"blue\">`Minería de Textos` </font>\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13a0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180e4d3",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Preprocesamiento** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c50694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # Lowercase and remove punctuation (keep whitespace)\n",
    "    text = text.lower()\n",
    "    # remove all characters that are not alphanumeric or whitespace\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    # collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e7791f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(tokens: List[str], min_freq=5, max_vocab=50000) -> Tuple[Dict[str,int], Dict[int,str], collections.Counter]:\n",
    "    freq = collections.Counter(tokens)\n",
    "    # remove rare words\n",
    "    freq = {w:c for w,c in freq.items() if c >= min_freq}\n",
    "    # sort by frequency\n",
    "    sorted_items = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_items = sorted_items[:max_vocab]\n",
    "    vocab = {w:i+2 for i,(w,_) in enumerate(sorted_items)}  # reserve 0 for PAD, 1 for UNK\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    inv_vocab = {i:w for w,i in vocab.items()}\n",
    "    # create a counter object for returned freq (only for kept words)\n",
    "    kept_counter = collections.Counter({w:c for w,c in sorted_items})\n",
    "    return vocab, inv_vocab, kept_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d8910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens: List[str], vocab: Dict[str,int]) -> List[int]:\n",
    "    unk = vocab.get('<UNK>')\n",
    "    return [vocab.get(t, unk) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3a038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_training_pairs(tokens: List[str],\n",
    "                            vocab: Dict[str,int],\n",
    "                            window_min=2,\n",
    "                            window_max=5,\n",
    "                            model_type='cbow') -> List[Tuple[List[int], int]]:\n",
    "    \"\"\"\n",
    "    Returns list of (context_indices, target_index) for CBOW\n",
    "    or list of (center_index, context_target_index) pairs for skip-gram.\n",
    "    For skip-gram we will make one training pair per context word (center -> context_word).\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    vocab_set = set(vocab.keys())\n",
    "    unk_idx = vocab['<UNK>']\n",
    "    N = len(tokens)\n",
    "    for i, w in enumerate(tokens):\n",
    "        # if word not in vocab (filtered), treat as UNK\n",
    "        center_idx = vocab.get(w, unk_idx)\n",
    "        window_size = random.randint(window_min, window_max)\n",
    "        left = max(0, i - window_size)\n",
    "        right = min(N, i + window_size + 1)\n",
    "        context = []\n",
    "        for j in range(left, right):\n",
    "            if j == i:\n",
    "                continue\n",
    "            context_word = tokens[j]\n",
    "            context_idx = vocab.get(context_word, unk_idx)\n",
    "            context.append(context_idx)\n",
    "        if not context:\n",
    "            continue\n",
    "        if model_type == 'cbow':\n",
    "            # store context (list) and center\n",
    "            pairs.append((context, center_idx))\n",
    "        else:  # skipgram\n",
    "            # produce one pair per context word (center -> that context target)\n",
    "            for ctx_idx in context:\n",
    "                pairs.append(([center_idx], ctx_idx))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdff81",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Pytorch Dataset** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c092229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[List[int], int]], model_type='cbow'):\n",
    "        self.pairs = pairs\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context_idxs, target_idx = self.pairs[idx]\n",
    "        return context_idxs, target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07418bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_cbow(batch):\n",
    "    # batch: list of (context_list, target)\n",
    "    contexts, targets = zip(*batch)\n",
    "    # contexts variable length -> we'll create tensor of shape (batch, max_len) and mask\n",
    "    max_len = max(len(c) for c in contexts)\n",
    "    batch_size = len(contexts)\n",
    "    contexts_tensor = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "    mask = torch.zeros((batch_size, max_len), dtype=torch.float32)\n",
    "    for i, c in enumerate(contexts):\n",
    "        contexts_tensor[i, :len(c)] = torch.tensor(c, dtype=torch.long)\n",
    "        mask[i, :len(c)] = 1.0\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "    return contexts_tensor, mask, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbc4ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_skipgram(batch):\n",
    "    # for skipgram we stored ([center_idx], target_idx) so contexts are singletons\n",
    "    centers = [c[0][0] for c,_ in batch]\n",
    "    targets = [t for _,t in batch]\n",
    "    centers_tensor = torch.tensor(centers, dtype=torch.long)\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "    return centers_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df0824",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Modelos** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6982bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecBaseModel(nn.Module):\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int=100):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # optional: initialize embeddings\n",
    "        nn.init.normal_(self.embed.weight, mean=0.0, std=0.1)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c46a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(Word2VecBaseModel):\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int=100):\n",
    "        super().__init__(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, contexts_idx: torch.LongTensor, mask: torch.FloatTensor):\n",
    "        # contexts_idx: (B, L)\n",
    "        # mask: (B, L) float where 1 means valid\n",
    "        emb = self.embed(contexts_idx)  # (B, L, D)\n",
    "        mask = mask.unsqueeze(-1)  # (B, L, 1)\n",
    "        emb = emb * mask  # zero-out pads\n",
    "        sum_emb = emb.sum(dim=1)  # (B, D)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1e-6)  # (B,1)\n",
    "        avg_emb = sum_emb / lengths  # (B, D)\n",
    "        logits = self.linear(avg_emb)  # (B, V)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af555d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(Word2VecBaseModel):\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int=100):\n",
    "        super().__init__(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, centers_idx: torch.LongTensor):\n",
    "        emb = self.embed(centers_idx)  # (B, D)\n",
    "        logits = self.linear(emb)  # (B, V)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007797c2",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Training helper** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37b1ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device, model_type='cbow'):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(dataloader, desc=\"train\", leave=False)\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        if model_type == 'cbow':\n",
    "            contexts_tensor, mask, targets = batch\n",
    "            contexts_tensor = contexts_tensor.to(device)\n",
    "            mask = mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits = model(contexts_tensor, mask)\n",
    "        else:\n",
    "            centers_tensor, targets = batch\n",
    "            centers_tensor = centers_tensor.to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits = model(centers_tensor)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03314a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_similar_words(embedding_weights: np.ndarray,\n",
    "                           word_to_idx: Dict[str,int],\n",
    "                           idx_to_word: Dict[int,str],\n",
    "                           anchor_words: List[str],\n",
    "                           topk=10):\n",
    "    # embeddings normalized\n",
    "    emb_norm = embedding_weights / (np.linalg.norm(embedding_weights, axis=1, keepdims=True) + 1e-9)\n",
    "    results = {}\n",
    "    for w in anchor_words:\n",
    "        idx = word_to_idx.get(w, None)\n",
    "        if idx is None:\n",
    "            results[w] = []\n",
    "            continue\n",
    "        vec = emb_norm[idx:idx+1]  # (1, D)\n",
    "        sims = (emb_norm @ vec.T).squeeze()  # (V,)\n",
    "        # remove self\n",
    "        sims[idx] = -np.inf\n",
    "        top_idx = np.argsort(-sims)[:topk]\n",
    "        results[w] = [(idx_to_word[i], float(sims[i])) for i in top_idx]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c27f1",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Visualizaciones** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a473449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_embeddings(embeddings_2d: np.ndarray, words: List[str], title=''):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.scatterplot(x=embeddings_2d[:,0], y=embeddings_2d[:,1], s=30)\n",
    "    for i,w in enumerate(words):\n",
    "        plt.text(embeddings_2d[i,0]+0.005, embeddings_2d[i,1]+0.005, w, fontsize=9)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5af9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_and_plot(embeddings: np.ndarray, idx_to_word: Dict[int,str], top_n=500, method='tsne', random_state=42, title=''):\n",
    "    # select top_n most frequent indices (assumes idx_to_word ordering not by freq; user should pass frequent list)\n",
    "    # Here we'll expect an ordered list of indices to keep\n",
    "    if method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, init='pca', random_state=random_state, perplexity=30)\n",
    "        emb2d = reducer.fit_transform(embeddings)\n",
    "    else:\n",
    "        reducer = umap.UMAP(n_components=2, random_state=random_state)\n",
    "        emb2d = reducer.fit_transform(embeddings)\n",
    "    return emb2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba4599",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Ejemplo de uso** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2b00190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(text_path: str,\n",
    "                 model_type='cbow',  # 'cbow' or 'skipgram'\n",
    "                 min_freq=5,\n",
    "                 max_vocab=50000,\n",
    "                 embedding_dim=100,\n",
    "                 epochs=5,\n",
    "                 batch_size=1024,\n",
    "                 window_min=2,\n",
    "                 window_max=5,\n",
    "                 device=None):\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # 1. Read file\n",
    "    with open(text_path, 'r', encoding='utf-8') as f:\n",
    "        raw = f.read()\n",
    "    print(\"Raw size (chars):\", len(raw))\n",
    "\n",
    "    # 2. Normalize and tokenize by whitespace\n",
    "    print(\"Normalizing...\")\n",
    "    norm = normalize_text(raw)\n",
    "    tokens = norm.split()  # tokenization by whitespace\n",
    "    print(\"Total tokens:\", len(tokens))\n",
    "\n",
    "    # 3. Build vocabulary\n",
    "    print(\"Building vocab...\")\n",
    "    vocab, inv_vocab, kept_counts = build_vocab(tokens, min_freq=min_freq, max_vocab=max_vocab)\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Vocab size (after filtering+cap): {vocab_size}\")\n",
    "\n",
    "    # 4. Generate pairs (with randomized window per center)\n",
    "    print(\"Generating training pairs (this may take a while depending on corpus size)...\")\n",
    "    pairs = generate_training_pairs(tokens, vocab,\n",
    "                                    window_min=window_min,\n",
    "                                    window_max=window_max,\n",
    "                                    model_type=model_type)\n",
    "    print(\"Total training pairs:\", len(pairs))\n",
    "\n",
    "    # 5. Dataset / DataLoader\n",
    "    dataset = Word2VecDataset(pairs, model_type=model_type)\n",
    "    if model_type == 'cbow':\n",
    "        collate_fn = lambda b: collate_cbow(b)\n",
    "    else:\n",
    "        collate_fn = lambda b: collate_skipgram(b)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "\n",
    "    # 6. Model\n",
    "    model_cls = CBOWModel if model_type == 'cbow' else SkipGramModel\n",
    "    model = model_cls(vocab_size=vocab_size, embedding_dim=embedding_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 7. Train\n",
    "    for ep in range(1, epochs+1):\n",
    "        print(f\"Epoch {ep}/{epochs}\")\n",
    "        avg_loss = train_epoch(model, dataloader, optimizer, criterion, device, model_type=model_type)\n",
    "        print(f\"Epoch {ep} avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 8. Get embeddings\n",
    "    emb_weights = model.embed.weight.data.cpu().numpy()  # (V, D)\n",
    "\n",
    "    # 9. Evaluation: most similar for anchor words\n",
    "    idx_to_word = {i:w for w,i in vocab.items()}\n",
    "    anchor_words = ['king', 'queen', 'apple', 'run', 'doctor']  # adjust as desired\n",
    "    similar = evaluate_similar_words(emb_weights, vocab, idx_to_word, anchor_words, topk=10)\n",
    "    print(\"\\nMost similar words (cosine) — anchors:\", anchor_words)\n",
    "    for anchor, lst in similar.items():\n",
    "        print(f\"\\n{anchor}:\")\n",
    "        for w,score in lst:\n",
    "            print(f\"  {w} ({score:.4f})\")\n",
    "\n",
    "    # 10. Visualization (top frequent words)\n",
    "    # pick most frequent words from kept_counts — ensure we include words in vocab\n",
    "    most_common = [w for w,_ in kept_counts.most_common(500) if w in vocab][:500]\n",
    "    indices = [vocab[w] for w in most_common]\n",
    "    emb_subset = emb_weights[indices]  # (n, D)\n",
    "\n",
    "    # t-SNE\n",
    "    print(\"Reducing with t-SNE...\")\n",
    "    tsne_2d = TSNE(n_components=2, init='pca', random_state=42, perplexity=30).fit_transform(emb_subset)\n",
    "    plot_2d_embeddings(tsne_2d, most_common, title=f\"{model_type.upper()} embeddings t-SNE\")\n",
    "\n",
    "    # UMAP\n",
    "    print(\"Reducing with UMAP...\")\n",
    "    umap_2d = umap.UMAP(n_components=2, random_state=42).fit_transform(emb_subset)\n",
    "    plot_2d_embeddings(umap_2d, most_common, title=f\"{model_type.upper()} embeddings UMAP\")\n",
    "\n",
    "    # Return useful artifacts\n",
    "    return {\n",
    "        'model': model,\n",
    "        'vocab': vocab,\n",
    "        'inv_vocab': idx_to_word,\n",
    "        'embeddings': emb_weights,\n",
    "        'similar': similar,\n",
    "        'most_common': most_common,\n",
    "        'tsne_2d': tsne_2d,\n",
    "        'umap_2d': umap_2d\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a16052",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Como Script** </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e19b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Raw size (chars): 100000000\n",
      "Normalizing...\n",
      "Total tokens: 17005207\n",
      "Building vocab...\n",
      "Vocab size (after filtering+cap): 50002\n",
      "Generating training pairs (this may take a while depending on corpus size)...\n",
      "Total training pairs: 17005207\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 0/33214 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "result = run_pipeline(\n",
    "    text_path=\"text8\",   # o \"text 8\" si tu archivo tiene espacio\n",
    "    model_type=\"cbow\",   # o \"skipgram\"\n",
    "    min_freq=5,\n",
    "    max_vocab=50000,\n",
    "    embedding_dim=100,\n",
    "    epochs=2,            # pon menos para probar\n",
    "    batch_size=512,\n",
    "    window_min=2,\n",
    "    window_max=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iteso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
